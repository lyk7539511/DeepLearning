{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"FA85DE7706AD4855BCCBE3546ED2BA6B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# Numpy基础\n\n\n \n大家好，这里是你们的第一个作业，即使你之前没有用过python，这个作业也会帮助你熟悉接下来会用到的功能\n\n**操作指南:**\n* 请使用python3\n* 避免使用for循环，除非题目里要求\n* 不要更改(# GRADED FUNCTION [function name])的注释\n* 写完代码，运行下面的cell确认你的输出是对的\n\n**做完这个作业，你能学会：**\n* 用ipython notebook\n* 用numpy，包括函数调用及向量矩阵运算\n* 理解“广播”的概念\n* 向量化代码\n\n让我们开始吧！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"26A9F43C6BC149108AC04115736A97A2","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n## 关于iPython笔记本##\n\niPython Notebook是嵌入在网页中的交互式编码环境。 你将在此类中使用iPython笔记本。 你只需要在### START CODE HERE ###和### END CODE HERE ###注释之间编写代码。 编写代码后，通过按“ SHIFT” +“ ENTER”或单击笔记本上部栏中的“Run Cell”来运行该单元块。\n\n我们通常会在注释中说明“（≈X行代码）”，以告诉你需要编写多少行代码。 当然这只是一个粗略的估计，当你的代码更长或更短时，也不用在意。\n\n**练习**：运行下面的两个单元格中将test设为“ Hello World”，并输出“ Hello World”。\n\n"},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"B41C49C496B742F48158527DCF900D32","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"### START CODE HERE ### (≈ 1 line of code)\ntest = \"Hello World\"\n### END CODE HERE ###"},{"cell_type":"code","execution_count":2,"metadata":{"slideshow":{"slide_type":"slide"},"id":"70CC13D6D5904D9D91140E6160ECA9E4","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"test: Hello World\n","name":"stdout"}],"source":"print (\"test: \" + test)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"29D9E62C77C0486583C33852E2BFCCC6","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\ntest: Hello World"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8A7697E3913047AC889E1272AA41A675","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你需要记住以下几点**：\n-使用SHIFT + ENTER（或点击“Run cell”）运行单元格\n-使用Python 3在指定区域内编写代码\n-请勿在指定区域以外修改代码"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"64A65C0DEFAA49D3820B081E60C8A6AC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 1-使用numpy构建基本函数##\n\nNumpy是Python中主要的科学计算包。它由一个[大型社区](www.numpy.org)维护。在本练习中，你将学习一些关键的numpy函数，例如np.exp，np.log和np.reshape。你需要知道如何使用这些函数去完成将来的练习。\n\n### 1.1- sigmoid function和np.exp（）###\n\n在使用np.exp（）之前，你将使用math.exp（）实现Sigmoid函数。然后，你将知道为什么np.exp（）比math.exp（）更可取。\n\n**练习**：构建一个返回实数x的sigmoid的函数。将math.exp（x）用于指数函数。\n\n**提示**：\n$sigmoid(x) = \\frac{1}{1+e^{-x}}$有时也称为逻辑函数。它是一种非线性函数，即可用于机器学习（逻辑回归），也能用于深度学习。\n\n要引用特定程序包的函数，可以使用package_name.function（）对其进行调用。运行下面的代码查看带有math.exp（）的示例。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"13197F93929347EC9677020D27C701FD","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: basic_sigmoid\n\nimport math\n\ndef basic_sigmoid(x):\n    \"\"\"\n    Compute sigmoid of x.\n\n    Arguments:\n    x -- A scalar\n\n    Return:\n    s -- sigmoid(x)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    s = 1/(1 + math.exp(-x))\n    ### END CODE HERE ###\n    \n    return s"},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"},"id":"0C4AEA0860CB43D48486C0620A4959CA","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"0.9525741268224334"},"transient":{},"execution_count":4}],"source":"basic_sigmoid(3)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6761A4D686144E1C8794D7FCAFC96A7A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \n0.9525741268224334"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"EAD37E92BCD9401982CE86201D00C6C3","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":" 因为函数的输入是实数，所以我们很少在深度学习中使用“math”库。 而深度学习中主要使用的是矩阵和向量，因此numpy更为实用。"},{"cell_type":"code","execution_count":5,"metadata":{"slideshow":{"slide_type":"slide"},"id":"5598896D67B440A996340AC299EE93DC","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"bad operand type for unary -: 'list'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8ccefa5bf989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbasic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# you will see this give an error when you run it, because x is a vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-28be2b65a92e>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"]}],"source":"### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\nx = [1, 2, 3]\nbasic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"55EAE3A2BA7940B887B4E3D61E22B887","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n如果 $x = (x_1, x_2, ..., x_n)$ 是行向量，则$np.exp(x)$会将指数函数应用于x的每个元素。 因此，输出为：$np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"},{"cell_type":"code","execution_count":7,"metadata":{"slideshow":{"slide_type":"slide"},"id":"93DF8131937144A58DC3538838033CC5","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"[ 2.71828183  7.3890561  20.08553692]\n","name":"stdout"}],"source":"import numpy as np\n\n# example of np.exp\nx = np.array([1, 2, 3])\nprint(np.exp(x)) # result is (exp(1), exp(2), exp(3))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"03D909A75013446F86746FCE634B77B0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"如果x是向量，则$s = x + 3$或$s = \\frac{1}{x}$之类的Python运算将输出与x维度大小相同的向量s。"},{"cell_type":"code","execution_count":8,"metadata":{"slideshow":{"slide_type":"slide"},"id":"13E01BCA16E34A75876915D5C4778E6F","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"[4 5 6]\n","name":"stdout"}],"source":"# example of vector operation\nx = np.array([1, 2, 3])\nprint (x + 3)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6824776A948A41BA85DFB32DA6A6D57C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"你可以查看[官方文档](https://docs.scipy.org/doc/numpy-1.10.1/reference/genic/numpy.exp.html )去获取更多有关numpy函数的信息。\n\n也可以在笔记本中创建一个新单元格，并编写`np.exp?` （例如）快速访问文档。\n\n**练习**：使用numpy实现sigmoid函数。\n\n**说明**：x可以是实数，向量或矩阵。 我们在numpy中使用的表示向量、矩阵等的数据结构称为numpy数组。现阶段你只需了解这些就已足够。\n\n\n$$\n\\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n    x_1  \\\\\n    x_2  \\\\\n    ...  \\\\\n    x_n  \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n    \\frac{1}{1+e^{-x_1}}  \\\\\n    \\frac{1}{1+e^{-x_2}}  \\\\\n    ...  \\\\\n    \\frac{1}{1+e^{-x_n}}  \\\\\n\\end{pmatrix}\\tag{1} \n$$"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"153129429410443383055A9C10B18EDE","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: sigmoid\n\nimport numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n\ndef sigmoid(x):\n    \"\"\"\n    Compute the sigmoid of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size\n\n    Return:\n    s -- sigmoid(x)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    s = 1 / (1 + np.exp(-x))\n    ### END CODE HERE ###\n    \n    return s"},{"cell_type":"code","execution_count":10,"metadata":{"slideshow":{"slide_type":"slide"},"id":"5E1166009E4242A28786C604276787F6","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"array([0.73105858, 0.88079708, 0.95257413])"},"transient":{},"execution_count":10}],"source":"x = np.array([1, 2, 3])\nsigmoid(x)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"968DBD5DF34846CF8D518F0B07C35C84","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \narray([ 0.73105858,  0.88079708,  0.95257413])"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"46786CC4D31C417086904CD47128E389","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.2- Sigmoid gradient\n\n正如你在教程中所看到的，我们需要计算梯度来使用反向传播优化损失函数。 让我们开始编写第一个梯度函数吧。\n\n**练习**：创建函数sigmoid_grad（）计算sigmoid函数相对于其输入x的梯度。 公式为：$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$\n我们通常分两步编写此函数代码：\n1.将s设为x的sigmoid。 你可能会发现sigmoid（x）函数很方便。\n2.计算$\\sigma'(x) = s(1-s)$"},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"BC8B8B905029410D9D30D4C57441F9E2","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: sigmoid_derivative\n\ndef sigmoid_derivative(x):\n    \"\"\"\n    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n    \n    Arguments:\n    x -- A scalar or numpy array\n\n    Return:\n    ds -- Your computed gradient.\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 2 lines of code)\n    s = sigmoid(x)\n    ds = s * (1 - s)\n    ### END CODE HERE ###\n    \n    return ds"},{"cell_type":"code","execution_count":11,"metadata":{"slideshow":{"slide_type":"slide"},"id":"FF1751C1594B4F2F86B6300507AD535E","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n","name":"stdout"}],"source":"x = np.array([1, 2, 3])\nprint (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"42F341FEFC7943FE80BECA976745B23C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nsigmoid_derivative([1,2,3]) = [ 0.19661193  0.10499359  0.04517666]\n\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E42C212DAC02467795A7274C23D9421B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.3- 重塑数组###\n\n深度学习中两个常用的numpy函数是[np.shape](https://docs.scipy.org/doc/numpy/reference/generation/numpy.ndarray.shape.html)和[np.reshape()](https://docs.scipy.org/doc/numpy/reference/generation/numpy.reshape.html)。\n-X.shape用于获取矩阵/向量X的shape（维度）。\n-X.reshape（...）用于将X重塑为其他尺寸。\n\n例如，在计算机科学中，图像由shape为$(length, height, depth = 3)$的3D数组表示。但是，当你读取图像作为算法的输入时，会将其转换为维度为$(length*height*3, 1)$的向量。换句话说，将3D阵列“展开”或重塑为1D向量。\n\n![Image Name](https://cdn.kesci.com/upload/image/q15iureyxk.png?imageView2/0/w/960/h/960)\n\n**练习**：实现`image2vector()` ,该输入采用维度为(length, height, 3)的输入，并返回维度为(length\\*height\\*3, 1)的向量。例如，如果你想将形为（a，b，c）的数组v重塑为维度为(a*b, 3)的向量，则可以执行以下操作：\n``` python\nv = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n```\n-请不要将图像的尺寸硬编码为常数。而是通过image.shape [0]等来查找所需的数量。\n"},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"528A408A910249658E0043ACF6CA7303","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: image2vector\ndef image2vector(image):\n    \"\"\"\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n    \n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n    ### END CODE HERE ###\n    \n    return v"},{"cell_type":"code","execution_count":13,"metadata":{"slideshow":{"slide_type":"slide"},"id":"7C5389497EC84A199F3BA466E9EB5721","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"image2vector(image) = [[0.67826139]\n [0.29380381]\n [0.90714982]\n [0.52835647]\n [0.4215251 ]\n [0.45017551]\n [0.92814219]\n [0.96677647]\n [0.85304703]\n [0.52351845]\n [0.19981397]\n [0.27417313]\n [0.60659855]\n [0.00533165]\n [0.10820313]\n [0.49978937]\n [0.34144279]\n [0.94630077]]\n","name":"stdout"}],"source":"# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\nimage = np.array([[[ 0.67826139,  0.29380381],\n        [ 0.90714982,  0.52835647],\n        [ 0.4215251 ,  0.45017551]],\n\n       [[ 0.92814219,  0.96677647],\n        [ 0.85304703,  0.52351845],\n        [ 0.19981397,  0.27417313]],\n\n       [[ 0.60659855,  0.00533165],\n        [ 0.10820313,  0.49978937],\n        [ 0.34144279,  0.94630077]]])\n\nprint (\"image2vector(image) = \" + str(image2vector(image)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2025769276504A74ABCCF967C649C544","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nimage2vector(image)=[[ 0.67826139]\n [ 0.29380381]\n [ 0.90714982]\n [ 0.52835647]\n [ 0.4215251 ]\n [ 0.45017551]\n [ 0.92814219]\n [ 0.96677647]\n [ 0.85304703]\n [ 0.52351845]\n [ 0.19981397]\n [ 0.27417313]\n [ 0.60659855]\n [ 0.00533165]\n [ 0.10820313]\n [ 0.49978937]\n [ 0.34144279]\n [ 0.94630077]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7331F8F6B7174B6398A1D8BE6A8BD6DA","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.4- 行标准化\n\n我们在机器学习和深度学习中使用的另一种常见技术是对数据进行标准化。 由于归一化后梯度下降的收敛速度更快，通常会表现出更好的效果。 通过归一化，也就是将x更改为$\\frac{x}{\\| x\\|}$（将x的每个行向量除以其范数）。\n\n例如：\n\n$$\nx = \n\\begin{bmatrix}\n    0 & 3 & 4 \\\\\n    2 & 6 & 4 \\\\\n\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n    5 \\\\\n    \\sqrt{56} \\\\\n\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n\\end{bmatrix}\\tag{5}\n$$ \n\n请注意，你可以划分不同大小的矩阵，以获得更好的效果：这称为broadcasting，我们将在第5部分中学习它。\n\n\n**练习**：执行 normalizeRows（）来标准化矩阵的行。 将此函数应用于输入矩阵x之后，x的每一行应为单位长度（即长度为1）向量。"},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"6C7EDAE6EB2F4AF3A4F076CB96EDD80B","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: normalizeRows\n\ndef normalizeRows(x):\n    \"\"\"\n    Implement a function that normalizes each row of the matrix x (to have unit length).\n    \n    Argument:\n    x -- A numpy matrix of shape (n, m)\n    \n    Returns:\n    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 2 lines of code)\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, axis = 1, keepdims = True)\n    \n    # Divide x by its norm.\n    x = x / x_norm\n    ### END CODE HERE ###\n\n    return x"},{"cell_type":"code","execution_count":15,"metadata":{"slideshow":{"slide_type":"slide"},"id":"4E132DD5C9F94514AEFEE1407F67283A","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"normalizeRows(x) = [[0.         0.6        0.8       ]\n [0.13736056 0.82416338 0.54944226]]\n","name":"stdout"}],"source":"x = np.array([\n    [0, 3, 4],\n    [1, 6, 4]])\nprint(\"normalizeRows(x) = \" + str(normalizeRows(x)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"17E0FAD1ECA7424C89319E70B5806AA0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nnormalizeRows(x)=[[ 0.          0.6         0.8       ]\n [ 0.13736056  0.82416338  0.54944226]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"10EA1B15487A492E8EF9BBB28DB78CEC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**注意**：\n在normalizeRows（）中，你可以尝试print查看 x_norm和x的维度，然后重新运行练习cell。 你会发现它们具有不同的w维度。 鉴于x_norm采用x的每一行的范数，这是正常的。 因此，x_norm具有相同的行数，但只有1列。 那么，当你将x除以x_norm时，它是如何工作的？ 这就是所谓的广播broadcasting，我们现在将讨论它！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"94084DF992A84FC8AB530A5FD58395A3","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 1.5- 广播和softmax函数####\n在numpy中要理解的一个非常重要的概念是“广播”。 这对于在不同形状的数组之间执行数学运算非常有用。 有关广播的完整详细信息，你可以阅读官方的[broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"48C69607D73F42C288AEEA23A145E7D3","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**练习**:  使用numpy实现softmax函数。 你可以将softmax理解为算法需要对两个或多个类进行分类时使用的标准化函数。 你将在本专业的第二门课中了解有关softmax的更多信息。\n**操作指南**:\n- \n$$\n\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n    x_1  &&\n    x_2 &&\n    ...  &&\n    x_n  \n\\end{bmatrix}) = \\begin{bmatrix}\n     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n    ...  &&\n    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n\\end{bmatrix} \n$$ \n\n- \n$\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  }x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have:  \n$$\nsoftmax(x) = softmax\\begin{bmatrix}\n    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n\\end{bmatrix} = \\begin{bmatrix}\n    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n\\end{bmatrix} = \\begin{pmatrix}\n    softmax\\text{(first row of x)}  \\\\\n    softmax\\text{(second row of x)} \\\\\n    ...  \\\\\n    softmax\\text{(last row of x)} \\\\\n\\end{pmatrix} \n$$"},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"1DB5F862EDE345F685F0EB1F2B3A3D3E","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: softmax\n\ndef softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 3 lines of code)\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n    \n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n\n    ### END CODE HERE ###\n    \n    return s"},{"cell_type":"code","execution_count":17,"metadata":{"slideshow":{"slide_type":"slide"},"id":"111A867C767A4E17A9D259ED49463DDE","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n  1.21052389e-04]\n [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n  8.01252314e-04]]\n","name":"stdout"}],"source":"x = np.array([\n    [9, 2, 5, 0, 0],\n    [7, 5, 0, 0 ,0]])\nprint(\"softmax(x) = \" + str(softmax(x)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D77D2EF7AF56401F8746110FE1B76362","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nsoftmax(x)=[[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n    1.21052389e-04]\n [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n    8.01252314e-04]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"919BFE5664B54EFA8B263FDFD6734D7A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**注意**：\n-如果你在上方输出 x_exp，x_sum和s的维度并重新运行练习单元，则会看到x_sum的纬度为（2,1），而x_exp和s的维度为（2,5）。 **x_exp/x_sum** 可以使用python广播。\n\n恭喜你！ 你现在已经对python numpy有了很好的理解，并实现了一些将在深度学习中用到的功能。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"ED53EB1660A243F9A99237640C85D388","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你需要记住的内容：**\n-np.exp（x）适用于任何np.array x并将指数函数应用于每个坐标\n-sigmoid函数及其梯度\n-image2vector通常用于深度学习\n-np.reshape被广泛使用。 保持矩阵/向量尺寸不变有助于我们消除许多错误。\n-numpy具有高效的内置功能\n-broadcasting非常有用"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"4938198E964C44F79232EC1CACDBFBF1","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2-向量化"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B740982540F24ACB80C1B24076D67866","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"在深度学习中，通常需要处理非常大的数据集。 因此，非计算最佳函数可能会成为算法中的巨大瓶颈，并可能使模型运行一段时间。 为了确保代码的高效计算，我们将使用向量化。 例如，尝试区分点/外部/元素乘积之间的区别。"},{"cell_type":"code","execution_count":18,"metadata":{"slideshow":{"slide_type":"slide"},"id":"3AEDE9C1F25346EF81C056C5A19E93C7","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"dot = 278\n ----- Computation time = 0.08447300000002933ms\nouter = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n ----- Computation time = 0.22404599999992225ms\nelementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n ----- Computation time = 0.10582699999994727ms\ngdot = [26.19713459 12.20793127 23.40980652]\n ----- Computation time = 0.15482099999997168ms\n","name":"stdout"}],"source":"import time\n\nx1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n\n### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\ntic = time.process_time()\ndot = 0\nfor i in range(len(x1)):\n    dot+= x1[i]*x2[i]\ntoc = time.process_time()\nprint (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\ntic = time.process_time()\nouter = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\nfor i in range(len(x1)):\n    for j in range(len(x2)):\n        outer[i,j] = x1[i]*x2[j]\ntoc = time.process_time()\nprint (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### CLASSIC ELEMENTWISE IMPLEMENTATION ###\ntic = time.process_time()\nmul = np.zeros(len(x1))\nfor i in range(len(x1)):\n    mul[i] = x1[i]*x2[i]\ntoc = time.process_time()\nprint (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\nW = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\ntic = time.process_time()\ngdot = np.zeros(W.shape[0])\nfor i in range(W.shape[0]):\n    for j in range(len(x1)):\n        gdot[i] += W[i,j]*x1[j]\ntoc = time.process_time()\nprint (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"},{"cell_type":"code","execution_count":24,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E0703051C9054F3880B0035CC322305C","jupyter":{},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dot = 278\n"," ----- Computation time = 0.0ms\n","outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"," ----- Computation time = 0.0ms\n","elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n"," ----- Computation time = 0.0ms\n","gdot = [ 21.57937154  22.58814194  13.70092277]\n"," ----- Computation time = 0.0ms\n"]}],"source":"x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n\n### VECTORIZED DOT PRODUCT OF VECTORS ###\ntic = time.process_time()\ndot = np.dot(x1,x2)\ntoc = time.process_time()\nprint (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### VECTORIZED OUTER PRODUCT ###\ntic = time.process_time()\nouter = np.outer(x1,x2)\ntoc = time.process_time()\nprint (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### VECTORIZED ELEMENTWISE MULTIPLICATION ###\ntic = time.process_time()\nmul = np.multiply(x1,x2)\ntoc = time.process_time()\nprint (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n\n### VECTORIZED GENERAL DOT PRODUCT ###\ntic = time.process_time()\ndot = np.dot(W,x1)\ntoc = time.process_time()\nprint (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3C1FA11688AA4D4D87C543D70069831F","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"你可能注意到了，向量化的实现更加简洁高效。 对于更大的向量/矩阵，运行时间的差异变得更大。\n\n**注意** 不同于`np.multiply()`和`*` 操作符（相当于Matlab / Octave中的 `.*`）执行逐元素的乘法，`np.dot()`执行的是矩阵-矩阵或矩阵向量乘法，\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E5554FCC79764444931E9D194A8F14C9","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 2.1实现L1和L2损失函数\n\n**练习**：实现L1损失函数的Numpy向量化版本。 我们会发现函数abs（x）（x的绝对值）很有用。\n\n**提示**：\n-损失函数用于评估模型的性能。 损失越大，预测($ \\hat{y} $) 与真实值($y$)的差异也就越大。 在深度学习中，我们使用诸如Gradient Descent之类的优化算法来训练模型并最大程度地降低成本。\n-L1损失函数定义为：\n$$\n\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}\n$$\n"},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"B7349AD752FE46868295D0B4C6634897","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: L1\n\ndef L1(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L1 loss function defined above\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    loss = np.sum(np.abs(y - yhat))\n    ### END CODE HERE ###\n    \n    return loss"},{"cell_type":"code","execution_count":20,"metadata":{"slideshow":{"slide_type":"slide"},"id":"349A8DA5BDCA4E1B8C8634E3A4AE4F90","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"L1 = 1.1\n","name":"stdout"}],"source":"yhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L1 = \" + str(L1(yhat,y)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F00F0886B3D14C9586EE0D7504F59159","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nL1=1.1"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1F30F8694E5548EB9D21E58443A7EA0C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**练习**：实现L2损失函数的Numpy向量化版本。 有好几种方法可以实现L2损失函数，但是还是np.dot（）函数更好用。 提醒一下，如果$x = [x_1, x_2, ..., x_n]$，则`np.dot（x，x）`= $\\sum_{j=0}^n x_j^{2}$。\n\n-L2损失函数定义为：\n$$\n\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}\n$$\n"},{"cell_type":"code","execution_count":21,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"DCDF298996E54CCF8B1FD782DBAA43D3","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: L2\n\ndef L2(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L2 loss function defined above\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    loss = np.dot((y - yhat),(y - yhat).T)\n    ### END CODE HERE ###\n    \n    return loss"},{"cell_type":"code","execution_count":22,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9D51BE886F08419E8D81BB0B9B1C871C","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"L2 = 0.43\n","name":"stdout"}],"source":"yhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L2 = \" + str(L2(yhat,y)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"50084188E0014979A442EAB9C35620E5","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nL2=0.43"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"88C5D7D91F9742D6B864614AC22B492B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"祝贺你完成此教程练习。 我们希望这个小小的热身运动可以帮助你以后的工作，那将更加令人兴奋和有趣！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"EDCE88FB2D0546E0BF2B11B2F36DBA68","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你需要记住的内容：**\n-向量化在深度学习中非常重要， 它保证了计算的效率和清晰度。\n-了解L1和L2损失函数。\n-掌握诸多numpy函数，例如np.sum，np.dot，np.multiply，np.maximum等。"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"AE31263FB9354DEA8B2CF33EFD6FFE51","jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}