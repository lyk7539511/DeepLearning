{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"44C8026B2A3D4F7EAF5F8F13FA1C9228","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 用神经网络思想实现Logistic回归\n\n欢迎来到你的第一个编程作业！ 你将学习如何建立逻辑回归分类器用来识别猫。 这项作业将引导你逐步了解神经网络的思维方式，同时磨练你对深度学习的直觉。\n\n\n**说明：**\n除非指令中明确要求使用，否则请勿在代码中使用循环（for / while）。\n\n**你将学习以下内容：**\n* 建立学习算法的一般架构，包括：\n\t* \t初始化参数\n\t* \t计算损失函数及其梯度\n\t* \t使用优化算法（梯度下降）\n* 按正确的顺序将以上所有三个功能集成到一个主模型上。\n\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DF7CD12B09824F908222DD939CE5B7DB","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 1- 安装包##\n\n首先，让我们运行下面的单元格，以导入作业中所需的包。\n* [numpy](www.numpy.org) 是Python科学计算的基本包。\n* [h5py](http://www.h5py.org)是一个常用的包，可以处理存储为H5文件格式的数据集。\n* [matplotlib](http://matplotlib.org)是一个著名的Python图形库。\n* 这里最后通过[PIL]（http://www.pythonware.com/products/pil/）和[scipy](https://www.scipy.org/) 使用你自己的图片去测试模型效果。"},{"metadata":{"id":"11378DD8FA5648998394E0A00228381D","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearningai17761\n","name":"stdout"}],"source":"cd ../input/deeplearningai17761","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"AE41E025E50B45B295BB4C814339A154","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"C903C89DCA2B482BB715EA420B783EEA","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2- 问题概述##\n\n**问题说明**：你将获得一个包含以下内容的数据集（\"data.h5\"）：\n*      标记为cat（y = 1）或非cat（y = 0）的**m_train**训练图像集\n*      标记为cat或non-cat的**m_test**测试图像集\n*      图像维度为（num_px，num_px，3），其中3表示3个通道（RGB）。 因此，每个图像都是正方形（高度= num_px）和（宽度= num_px）。\n\n你将构建一个简单的图像识别算法，该算法可以将图片正确分类为猫和非猫。\n让我们熟悉一下数据集吧， 首先通过运行以下代码来加载数据。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"172CE9E4A8214F82B0392A74773C8F62","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3065A29BDD2F412B92534BC95F9A48F2","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"我们在图像数据集（训练和测试）的末尾添加了\"_orig\"，以便对其进行预处理。 预处理后，我们将得到train_set_x和test_set_x（标签train_set_y和test_set_y不需要任何预处理）。\n\ntrain_set_x_orig和test_set_x_orig的每一行都是代表图像的数组。 你可以通过运行以下代码来可视化示例。 还可以随意更改`index`值并重新运行以查看其他图像。"},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"},"id":"AD52F14FA9B642B989AFDA66C319B43A","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"y = [0], it's a 'non-cat' picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/AD52F14FA9B642B989AFDA66C319B43A/q17i4peyk8.png\">"},"transient":{}}],"source":"# Example of a picture\nindex = 5\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1AAA47CDF0944B31AE91376B1E774A65","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"深度学习中的许多报错都来自于矩阵/向量尺寸不匹配。 如果你可以保持矩阵/向量的尺寸不变，那么将消除大多错误。\n\n**练习：** 查找以下各项的值：\n*      m_train（训练集示例数量）\n*      m_test（测试集示例数量）\n*      num_px（=训练图像的高度=训练图像的宽度）\n\n请记住，“ train_set_x_orig”是一个维度为（m_train，num_px，num_px，3）的numpy数组。 例如，你可以通过编写“ train_set_x_orig.shape [0]”来访问“ m_train”。"},{"cell_type":"code","execution_count":6,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"A76A8CF63B524EDB8864C8B997F2217E","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Number of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n","name":"stdout"}],"source":"### START CODE HERE ### (≈ 3 lines of code)\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n### END CODE HERE ###\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C3E15E52444047F281369E10ADE371DB","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**：\n训练集数量：m_train = 209\n测试集数量：m_test = 50\n每个图像的高度/宽度：num_px = 64\n每个图像的大小：（64，64，3）\ntrain_set_x维度：（209、64、64、3）\ntrainsety维度：（1，209）\ntest_set_x维度：（50、64、64、3）\ntest_set_y维度：（1，50）"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"76DD435D2EDE4BDC8F332C9C2E67F67D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"为了方便起见，你现在应该以维度(num_px $*$ num_px $*$ 3, 1)的numpy数组重塑维度（num_px，num_px，3）的图像。 此后，我们的训练（和测试）数据集是一个numpy数组，其中每列代表一个展平的图像。 应该有m_train（和m_test）列。\n\n**练习：** 重塑训练和测试数据集，以便将大小（num_px，num_px，3）的图像展平为单个形状的向量(num\\_px $*$ num\\_px $*$ 3, 1)。\n\n当你想将维度为（a，b，c，d）的矩阵X展平为形状为(b$*$c$*$d, a)的矩阵X_flatten时的一个技巧是：\n```python\nX_flatten = X.reshape（X.shape [0]，-1）.T     ＃ 其中X.T是X的转置矩阵\n```"},{"cell_type":"code","execution_count":7,"metadata":{"slideshow":{"slide_type":"slide"},"id":"99FE56CF4FE142B98DD32763A7AFE3AE","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"train_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\nsanity check after reshaping: [17 31 56 22 33]\n","name":"stdout"}],"source":"# Reshape the training and test examples\n\n### START CODE HERE ### (≈ 2 lines of code)\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n### END CODE HERE ###\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\nprint (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"566C3435990A41928684827E578BA1EB","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**：\ntrain_set_x_flatten维度：（12288，209）\ntrainsety维度：（1，209）\ntest_set_x_flatten维度：（12288，50）\ntest_set_y维度：（1，50）\n重塑后的检查维度：[17 31 56 22 33]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"161C9D76A24A49028389C881855DBFEA","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"为了表示彩色图像，必须为每个像素指定红、绿、蓝色通道（RGB），因此像素值实际上是一个从0到255的三个数字的向量。\n\n机器学习中一个常见的预处理步骤是对数据集进行居中和标准化，这意味着你要从每个示例中减去整个numpy数组的均值，然后除以整个numpy数组的标准差。但是图片数据集则更为简单方便，并且只要将数据集的每一行除以255（像素通道的最大值），效果也差不多。\n\n在训练模型期间，你将要乘以权重并向一些初始输入添加偏差以观察神经元的激活。然后，使用反向梯度传播以训练模型。但是，让特征具有相似的范围以至渐变不会爆炸是非常重要的。具体内容我们将在后面的教程中详细学习！\n\n开始标准化我们的数据集吧！"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"909CA96ACD524D148A8FE51D78B7677A","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"train_set_x = train_set_x_flatten/255.\ntest_set_x = test_set_x_flatten/255."},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BA9892621923421A9CBF415F07BE4AB6","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你需要记住的内容：**\n\n预处理数据集的常见步骤是：\n* 找出数据的尺寸和维度（m_train，m_test，num_px等）\n* 重塑数据集，以使每个示例都是大小为（num_px \\ * num_px \\ * 3，1）的向量\n* “标准化”数据"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9FB29993EBAA4992B975D45824C03D7E","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3- 学习算法的一般架构##\n\n现在是时候设计一种简单的算法来区分猫图像和非猫图像了。\n\n你将使用神经网络思维方式建立Logistic回归。 下图说明了为什么“逻辑回归实际上是一个非常简单的神经网络！”\n\n![Image Name](https://cdn.kesci.com/upload/image/q15kx21spv.png?imageView2/0/w/960/h/960)\n\n**算法的数学表达式**：\n\nFor one example $x^{(i)}$:\n$z^{(i)} = w^T x^{(i)} + b \\tag{1}$\n$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$ \n$$\n\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\n$$\n\nThe cost is then computed by summing over all training examples:\n$$\nJ = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\n$$\n\t\t\n**关键步骤**：\n在本练习中，你将执行以下步骤：\n*      初始化模型参数\n*      通过最小化损失来学习模型的参数\n*      使用学习到的参数进行预测（在测试集上）\n*      分析结果并得出结论"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0EAC454E062C4D52A9118A7AAAAB0D67","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 4- 构建算法的各个部分##\n\n建立神经网络的主要步骤是：\n1.定义模型结构（例如输入特征的数量）\n2.初始化模型的参数\n3.循环：\n*      计算当前损失（正向传播）\n*      计算当前梯度（向后传播）\n*      更新参数（梯度下降）\n\n你通常会分别构建1-3，然后将它们集成到一个称为“ model（）”的函数中。\n\n### 4.1- 辅助函数\n\n**练习**：使用“Python基础”中的代码，实现`sigmoid（）`。 如上图所示，你需要计算$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ 去预测。 使用np.exp（）。\n"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"81F71A54ADCE45CDA2BDB4E996D7C673","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    ### START CODE HERE ### (≈ 1 line of code)\n    s = 1 / (1 + np.exp(-z))\n    ### END CODE HERE ###\n    \n    return s"},{"cell_type":"code","execution_count":10,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"CCFAA44915914F1E94DF5807B4D1E423","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"sigmoid([0, 2]) = [0.5        0.88079708]\n","name":"stdout"}],"source":"print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D007BFFA95D648199A2D3B57682D9482","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nsigmoid([0, 2]) = [0.5        0.88079708]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"36B63B1202A549F18C69D64841DDC6C8","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 4.2- 初始化参数\n\n**练习：** 在下面的单元格中实现参数初始化。 你必须将w初始化为零的向量。 如果你不知道要使用什么numpy函数，请在Numpy库的文档中查找np.zeros（）。"},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"8ABE2060998C43AC99916E833B6A64C6","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    w = np.zeros((dim, 1))\n    b = 0\n    ### END CODE HERE ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b"},{"cell_type":"code","execution_count":12,"metadata":{"slideshow":{"slide_type":"slide"},"id":"0CD547641AA44FD08D9AC53052002F7B","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"w = [[0.]\n [0.]]\nb = 0\n","name":"stdout"}],"source":"dim = 2\nw, b = initialize_with_zeros(dim)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C232F03FA0F74D739A014839C217AE65","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nw = [[0.]\n[0.]]\nb = 0\n\n\n对于图像输入，w的维度为(num_px $\\times$ num_px $\\times$ 3, 1)。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D7A2929C11B349718327F1DB79F0B585","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 4.3- 前向和后向传播\n\n现在，你的参数已初始化，你可以执行“向前”和“向后”传播步骤来学习参数。\n\n**练习：** 实现函数propagate（）来计算损失函数及其梯度。\n\n**提示**：\n\n正向传播：\n* 得到X\n* 计算$A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n* 计算损失函数：$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\n你将要使用到以下两个公式：\n$$\n\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\n$$\n"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"8DB1AFC11EC64A9080D263AB25DC88B9","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(w.T, X) + b)            # compute activation\n    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))         # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dw = 1 / m * np.dot(X, (A - Y).T)\n    db = 1 / m * np.sum(A - Y)\n    ### END CODE HERE ###\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost"},{"cell_type":"code","execution_count":14,"metadata":{"slideshow":{"slide_type":"slide"},"id":"77781A6736AA4E808AD78A68A66813CA","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"dw = [[0.99993216]\n [1.99980262]]\ndb = 0.49993523062470574\ncost = 6.000064773192205\n","name":"stdout"}],"source":"w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6E4A28DA2046408BBD18DE3D0FE8FA8D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\ndw = [[0.99993216]\n [1.99980262]]\ndb = 0.49993523062470574\ncost = 6.000064773192205"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8241762441A44A16A761EE664900765B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### d）优化函数\n* 初始化参数。\n* 计算损失函数及其梯度。\n* 使用梯度下降来更新参数。\n\n**练习：** 写下优化函数。 目标是通过最小化损失函数 $J$ 来学习 $w$ 和 $b$。 对于参数$\\theta$，更新规则为$\\theta = \\theta - \\alpha \\text{ }  d\\theta$，其中$\\alpha$是学习率。\n"},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"113344FF60D4474BBCF6E2711FD08CD7","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (≈ 1-4 lines of code)\n        ### START CODE HERE ### \n        grads, cost = propagate(w, b, X, Y)\n        ### END CODE HERE ###\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        ### START CODE HERE ###\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training examples\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs"},{"cell_type":"code","execution_count":16,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E74C2694CE6949F79921B49D1C1FD2B1","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"w = [[0.1124579 ]\n [0.23106775]]\nb = 1.5593049248448891\ndw = [[0.90158428]\n [1.76250842]]\ndb = 0.4304620716786828\n[6.000064773192205]\n","name":"stdout"}],"source":"params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(costs)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C27D7E49616F43BF80D68986171C5217","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nw = [[0.1124579 ]\n [0.23106775]]\nb = 1.5593049248448891\ndw = [[0.90158428]\n [1.76250842]]\ndb = 0.4304620716786828\n[6.000064773192205]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7BD13AF784F34A8B9463ED3784FFA873","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**练习：** 上一个函数将输出学习到的w和b。 我们能够使用w和b来预测数据集X的标签。实现`predict（）`函数。 预测分类有两个步骤：\n1.计算$\\hat{Y} = A = \\sigma(w^T X + b)$\n2.将a的项转换为0（如果激活<= 0.5）或1（如果激活> 0.5），并将预测结果存储在向量“ Y_prediction”中。 如果愿意，可以在for循环中使用if / else语句。"},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"D2036B868BF14D4D85D270A7B1B9DE20","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    ### START CODE HERE ### (≈ 1 line of code)\n    A = sigmoid(np.dot(w.T, X) + b)\n    ### END CODE HERE ###\n\n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        ### START CODE HERE ### (≈ 4 lines of code)\n        if A[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n        ### END CODE HERE ###\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction"},{"cell_type":"code","execution_count":18,"metadata":{"slideshow":{"slide_type":"slide"},"id":"1EFA81E9460C4CC8940C5699BB12CF30","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"predictions = [[1. 1.]]\n","name":"stdout"}],"source":"print (\"predictions = \" + str(predict(w, b, X)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"5F14960D06404F65B3E483CC4197FF46","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \npredictions = [[1. 1.]]\n"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"7433AC4720244B078F79F6D2B7B155AD","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你需要记住以下几点：**\n你已经实现了以下几个函数：\n* 初始化（w，b）\n* 迭代优化损失以学习参数（w，b）：\n\t*     计算损失及其梯度\n\t*     使用梯度下降更新参数\n* 使用学到的（w，b）来预测给定示例集的标签"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B1310A7D971047C1827D6550A5C0C5E4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 5- 将所有功能合并到模型中##\n\n现在，将所有构件（在上一部分中实现的功能）以正确的顺序放在一起，从而得到整体的模型结构。\n\n**练习：** 实现模型功能，使用以下符号：\n*      Y_prediction对测试集的预测\n*      Y_prediction_train对训练集的预测\n*      w，损失，optimize（）输出的梯度\n\t\t "},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"38BDB18490F544F1ABE51A02D60B42DE","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (≈ 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (≈ 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3466DB83E61D493C8A8E3F8C84F77283","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行以下单元格训练模型："},{"cell_type":"code","execution_count":20,"metadata":{"slideshow":{"slide_type":"slide"},"id":"362C4259BD8F44EA8D0E2513C387ADBF","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.693147\nCost after iteration 100: 0.584508\nCost after iteration 200: 0.466949\nCost after iteration 300: 0.376007\nCost after iteration 400: 0.331463\nCost after iteration 500: 0.303273\nCost after iteration 600: 0.279880\nCost after iteration 700: 0.260042\nCost after iteration 800: 0.242941\nCost after iteration 900: 0.228004\nCost after iteration 1000: 0.214820\nCost after iteration 1100: 0.203078\nCost after iteration 1200: 0.192544\nCost after iteration 1300: 0.183033\nCost after iteration 1400: 0.174399\nCost after iteration 1500: 0.166521\nCost after iteration 1600: 0.159305\nCost after iteration 1700: 0.152667\nCost after iteration 1800: 0.146542\nCost after iteration 1900: 0.140872\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n","name":"stdout"}],"source":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"40B908265E7A4C90B4343C4E631A3288","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"E791DEB85B214A12A0A506E43B9CF4B4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**评价**：训练准确性接近100％。 这是一个很好的情况：你的模型正在运行，并且具有足够的容量来适合训练数据。 测试误差为68％。 考虑到我们使用的数据集很小，并且逻辑回归是线性分类器，对于这个简单的模型来说，这实际上还不错。 但请放心，下周你将建立一个更好的分类器！\n\n此外，你会看到该模型明显适合训练数据。 在本专业的稍后部分，你将学习如何减少过度拟合，例如通过使用正则化。 使用下面的代码（并更改`index`变量），你可以查看测试集图片上的预测。"},{"cell_type":"code","execution_count":21,"metadata":{"slideshow":{"slide_type":"slide"},"id":"ED09E8724F694DBFAC6E963D8FD68D6F","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"y = 1, you predicted that it is a \"cat\" picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/ED09E8724F694DBFAC6E963D8FD68D6F/q15mtru0dr.png\">"},"transient":{}}],"source":"# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"061B89DE7D6A474483AE16B1C8168252","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"让我们绘制损失函数和梯度吧。"},{"cell_type":"code","execution_count":22,"metadata":{"slideshow":{"slide_type":"slide"},"id":"6BE9377DED634AB690C30677A1C2E2B3","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/6BE9377DED634AB690C30677A1C2E2B3/q15mtsthx1.png\">"},"transient":{}}],"source":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"24E878284D6C49A58600AFF680DAF014","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**解释**：\n损失下降表明正在学习参数。 但是，你看到可以在训练集上训练更多模型。 尝试增加上面单元格中的迭代次数，然后重新运行这些单元格。 你可能会看到训练集准确性提高了，但是测试集准确性却降低了。 这称为过度拟合。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B656750B8A294A31827503E052A9205E","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n## 6- 进一步分析（可选练习）##\n\n祝贺你建立了第一个图像分类模型。 让我们对其进行进一步分析，并研究如何选择学习率$\\alpha$。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A81C3149213B4D7E880FEDBB66C7D637","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"#### 学习率的选择 ####\n\n**提醒**：\n为了使梯度下降起作用，你必须明智地选择学习率。 学习率$\\alpha$决定我们更新参数的速度。 如果学习率太大，我们可能会“超出”最佳值。 同样，如果太小，将需要更多的迭代才能收敛到最佳值。 这也是为什么调整好学习率至关重要。\n\n让我们将模型的学习曲线与选择的几种学习率进行比较。 运行下面的单元格。 这大约需要1分钟。 还可以尝试与我们初始化要包含的“ learning_rates”变量的三个值不同的值，然后看看会发生什么。"},{"cell_type":"code","execution_count":23,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E351227574C845758992A95BE3F10621","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"learning rate is: 0.01\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n-------------------------------------------------------\n\nlearning rate is: 0.001\ntrain accuracy: 88.99521531100478 %\ntest accuracy: 64.0 %\n\n-------------------------------------------------------\n\nlearning rate is: 0.0001\ntrain accuracy: 68.42105263157895 %\ntest accuracy: 36.0 %\n\n-------------------------------------------------------\n\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/E351227574C845758992A95BE3F10621/q15mtyn7i3.png\">"},"transient":{}}],"source":"learning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"learning rate is: \" + str(i))\n    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2F6435CF24AB41BD80C89981EB66D916","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\t\t\n**解释**：\n* 不同的学习率会带来不同的损失，因此会有不同的预测结果。\n* 如果学习率太大（0.01），则成本可能会上下波动。 它甚至可能会发散（尽管在此示例中，使用0.01最终仍会以较高的损失值获得收益）。\n* 较低的损失并不意味着模型效果很好。当训练精度比测试精度高很多时，就会发生过拟合情况。\n* 在深度学习中，我们通常建议你：\n\t*      选择好能最小化损失函数的学习率。\n\t*      如果模型过度拟合，请使用其他方法来减少过度拟合。 （我们将在后面的教程中讨论。）\n"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"34CE567D3E0246DE8F324164A0AFF6BE","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n## 7-使用自己的图像进行测试（可选练习）##\n\n祝贺你完成此作业。 你可以使用自己的图像并查看模型的预测输出。 要做到这一点：\n     1.单击此笔记本上部栏中的 \"File\"，然后单击\"Open\" 以在Coursera Hub上运行。\n     2.将图像添加到Jupyter Notebook的目录中，在\"images\"文件夹中\n     3.在以下代码中更改图像的名称\n     4.运行代码，检查算法是否正确（1 = cat，0 = non-cat）！"},{"cell_type":"code","execution_count":24,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"5A8BBAE5BFD2497E8D5A1D2EA24DDE74","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\nUse Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n  \n","name":"stderr"},{"output_type":"stream","text":"y = 1.0, your algorithm predicts a \"cat\" picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5A8BBAE5BFD2497E8D5A1D2EA24DDE74/q15mtz9svb.png\">"},"transient":{}}],"source":"## START CODE HERE ## (PUT YOUR IMAGE NAME) \n#my_image = \"cat_in_iran.jpg\"   # change this to the name of your image file \n## END CODE HERE ##\n\n# We preprocess the image to fit your algorithm.\nfname = '/home/kesci/input/deeplearningai17761/cat_in_iran.jpg'\nimage = np.array(plt.imread(fname))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\nmy_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n\nplt.imshow(image)\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A8AF7111C6794A15881AD6479D141F03","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**此作业要记住的内容：**\n\n1. 预处理数据集很重要。\n1. 如何实现每个函数：initialize（），propagation（），optimize（），并用此构建一个model（）。\n1. 调整学习速率（这是“超参数”的一个示例）可以对算法产生很大的影响。 你将在本课程的稍后部分看到更多示例！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6B3611425A824BF4AB72C67072D1D674","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"最后，如果你愿意，我们邀请你在此笔记本上尝试其他操作。 在尝试任何操作之前，请确保你正确提交。 提交后，你可以学习了解的包括：\n*      发挥学习率和迭代次数\n*      尝试不同的初始化方法并比较结果\n*      测试其他预处理（将数据居中，或将每行除以其标准偏差）"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A674A5B3D48E4F9C85B0869052F66100","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"参考书目：\n- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"},{"metadata":{"id":"09DA48672F2745678D6B067C6EE0799B","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}